{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "Program to read the input data and process it for the transformer model\n",
    "- Read the file and split into english and corresponding python code\n",
    "- format the python code into one line\n",
    "    - factor new line\n",
    "    - factor tabs\n",
    "\n",
    "Also build a formatter that converts the single line python code\n",
    "back to proper syntax for python. This will used for visualization\n",
    "and verifing the output. This will be used to evaluate the model.\n",
    "\n",
    "\n",
    "Challenges in the input data\n",
    "\n",
    "    - Messy start various options to segment the input data \n",
    "        #write ; #1 ; #  write\n",
    "    - clean input file\n",
    "        - removed # comments len < 10 & 20 after checking \n",
    "        - updated few manually\n",
    "        - remove comments in the code ???\n",
    "        # dd - remove pattern\n",
    "\n",
    "Segment the input file\n",
    "    identify marker - messy data\n",
    "    extract segment\n",
    "    extract english sentence within segment\n",
    "    extract python code within python\n",
    "    format python code to one line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpaCy in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (1.19.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (0.7.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (3.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (3.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (2.24.0)\n",
      "Requirement already satisfied: setuptools in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (8.0.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (0.3.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (2.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (4.47.0)\n",
      "Requirement already satisfied: jinja2 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (2.11.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (20.4)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (1.7.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (2.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (0.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from SpaCy) (1.0.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->SpaCy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->SpaCy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->SpaCy) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->SpaCy) (1.25.9)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->SpaCy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from jinja2->SpaCy) (1.1.1)\n",
      "Requirement already satisfied: six in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->SpaCy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->SpaCy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /Users/senthilvinayagam/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->SpaCy) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clean_data.txt', 'data_processing-v1.ipynb', 'data_processing.ipynb', 'out', 'p_data.csv', 'english_python_data.txt', 'analysis_shortv3.txt', 'analysis_shortv2.txt', 'clean_in10.txt', 'analysis.txt', 'clean_in10_20.txt', '.ipynb_checkpoints', 'analysis_short10.txt', 'analysis_short.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out dataframe\n",
    "out_df = pd.DataFrame(columns = ['src', 'python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count of # :42424\n"
     ]
    }
   ],
   "source": [
    "input_file = 'clean_data.txt'\n",
    "output_file = os.path.join('analysis_shortv3.txt')\n",
    "# first print all lines starting with # with count\n",
    "with open(output_file,'w') as out_file:\n",
    "    eng_buf = ''\n",
    "    py_buf = ''\n",
    "    samples = 0\n",
    "    with open(input_file) as in_file:\n",
    "        for idx,line in enumerate(in_file):\n",
    "            if line.startswith('#') and len(line) > 30:\n",
    "                samples = samples + 1\n",
    "                out_df.loc[len(out_df)] = [eng_buf,py_buf]\n",
    "                py_buf = ''\n",
    "                eng_buf = line\n",
    "                #print(line)\n",
    "                #out_file.write(line)\n",
    "            else :\n",
    "                py_buf = py_buf + line\n",
    "                #out_file.write(line)\n",
    "            #if samples > 10:\n",
    "                #break\n",
    "print(f\" Count of # :{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4428"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_df['python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace tabs with 4 spaces\n",
    "out_df['python']= out_df['python'].str.replace('\\t', '    ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean spaces \n",
    "# 3->4\n",
    "# 7->8\n",
    "# 11->12\n",
    "\n",
    "reg3s_pat = re.compile(r'(:?\\n)[\\s]{3}([\\w])')\n",
    "reg7s_pat = re.compile(r'(:?\\n)[\\s]{7}([\\w])')\n",
    "reg11s_pat = re.compile(r'(:?\\n)[\\s]{11}([\\w])')\n",
    "\n",
    "def regex_clean(val):\n",
    "\n",
    "    clean_py = reg3s_pat.sub(r'\\1    \\2', val)\n",
    "    clean_py = reg7s_pat.sub(r'\\1        \\2', clean_py)\n",
    "    clean_py = reg11s_pat.sub(r'\\1            \\2', clean_py)\n",
    "    \n",
    "    return clean_py\n",
    "\n",
    "out_df['trg'] = out_df['python'].apply(regex_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functiont to test\n",
    "\n",
    "reg_pat = re.compile(r':?\\n[\\s]{3}[\\w]')\n",
    "def regex_filter(val):\n",
    "    if val:\n",
    "        mo = re.search(reg_pat,val)\n",
    "        if mo:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df_filt = out_df[out_df['trg'].apply(regex_filter)]\n",
    "len(df_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tokenize_en(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n",
      "new : ['\\n', 'def', 'findTargetSumWays', '(', 'nums', ',', 'S', ')', ':', '\\n    ', 'count', '=', '0', '\\n    ', 'def', 'calculate', '(', 'nums', ',', 'i', ',', 'sum', ',', 'S', ')', ':', '\\n        ', 'nonlocal', 'count', '\\n        ', 'if', 'i', '==', 'len', '(', 'nums', ')', ':', '\\n            ', 'if', 'sum', '==', 'S', ':', '\\n                ', 'count', '+', '=', '1', '\\n        ', 'else', ':', '\\n            ', 'calculate', '(', 'nums', ',', 'i+1', ',', 'sum+', 'nums', '[', 'i', ']', ',', 'S', ')', '\\n            ', 'calculate', '(', 'nums', ',', 'i+1', ',', 'sum-', 'nums', '[', 'i', ']', ',', 'S', ')', '\\n\\n    ', 'calculate', '(', 'nums', ',', '0', ',', '0', ',', 'S', ')', '\\n    ', 'return', 'count', '\\n\\n\\n\\n\\n']\n",
      "old : ['\\n', 'def', 'findTargetSumWays(nums', ',', 'S', '):', '\\n    ', 'count', '=', '0', '\\n    ', 'def', 'calculate(nums', ',', 'i', ',', 'sum', ',', 'S', '):', '\\n        ', 'nonlocal', 'count', '\\n        ', 'if', 'i', '=', '=', 'len(nums', '):', '\\n            ', 'if', 'sum', '=', '=', 'S', ':', '\\n                ', 'count', '+', '=', '1', '\\n        ', 'else', ':', '\\n            ', 'calculate(nums', ',', 'i+1', ',', 'sum+', 'nums[i', ']', ',', 'S', ')', '\\n            ', 'calculate(nums', ',', 'i+1', ',', 'sum-', 'nums[i', ']', ',', 'S', ')', '\\n\\n    ', 'calculate(nums', ',', '0', ',', '0', ',', 'S', ')', '\\n    ', 'return', 'count', '\\n\\n\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import keyword\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import random\n",
    "\n",
    "idx = random.randint(0,len(out_df))\n",
    "print(idx)\n",
    "val = out_df['trg'].iloc[idx]\n",
    "\n",
    "kw_dict = {}\n",
    "for kw in keyword.kwlist:\n",
    "    kw_dict[kw]= [{\"ORTH\":kw}]\n",
    "\n",
    "# learn 4, 8 12 spaces\n",
    "special_tabs = ['\\\\n    ','\\\\n        ','\\\\n            ']\n",
    "for tab in special_tabs:\n",
    "    kw_dict[tab] = [{\"ORTH\":tab}]\n",
    "#kw_dict    \n",
    "\n",
    "special_cases = kw_dict\n",
    "prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
    "suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
    "infix_re = re.compile(r'''(==|>=|<=|!=|\\(|\\)|:|\"|\\[|\\]|=|,|')''')\n",
    "#infix_re = re.compile(r'''(==|>=|<=|!=|:|=|,)''')\n",
    "simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                #prefix_search=prefix_re.search,\n",
    "                                #suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer)\n",
    "                                #url_match=simple_url_re.match)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(val)\n",
    "print(\"new :\",[t.text for t in doc]) # ['hello', '-', 'world.', ':)']\n",
    "print(\"old :\",tokenize_en(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all keywords\n",
    "kw_dict = {}\n",
    "for kw in keyword.kwlist:\n",
    "    kw_dict[kw]= [{\"ORTH\":kw}]\n",
    "\n",
    "# learn 4, 8 12 spaces\n",
    "special_tabs = ['\\\\n    ','\\\\n        ','\\\\n            ']\n",
    "for tab in special_tabs:\n",
    "    kw_dict[tab] = [{\"ORTH\":tab}]\n",
    "#kw_dict    \n",
    "\n",
    "special_cases = kw_dict\n",
    "#prefix_re = re.compile(r'''^[\\[\\(\"']''')\n",
    "#suffix_re = re.compile(r'''[\\]\\)\"']$''')\n",
    "infix_re = re.compile(r'''(==|>=|<=|!=|\\(|\\)|:|\"|\\[|\\]|=|,|')''')\n",
    "#infix_re = re.compile(r'''(==|>=|<=|!=|:|=|,)''')\n",
    "\n",
    "\n",
    "def python_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "                                #prefix_search=prefix_re.search,\n",
    "                                #suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer)\n",
    "                                #url_match=simple_url_re.match)\n",
    "\n",
    "\n",
    "py_custom = python_tokenizer(spacy_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_py(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in py_custom(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new : ['\\n', 'def', 'findTargetSumWays', '(', 'nums', ',', 'S', ')', ':', '\\n    ', 'count', '=', '0', '\\n    ', 'def', 'calculate', '(', 'nums', ',', 'i', ',', 'sum', ',', 'S', ')', ':', '\\n        ', 'nonlocal', 'count', '\\n        ', 'if', 'i', '==', 'len', '(', 'nums', ')', ':', '\\n            ', 'if', 'sum', '==', 'S', ':', '\\n                ', 'count', '+', '=', '1', '\\n        ', 'else', ':', '\\n            ', 'calculate', '(', 'nums', ',', 'i+1', ',', 'sum+', 'nums', '[', 'i', ']', ',', 'S', ')', '\\n            ', 'calculate', '(', 'nums', ',', 'i+1', ',', 'sum-', 'nums', '[', 'i', ']', ',', 'S', ')', '\\n\\n    ', 'calculate', '(', 'nums', ',', '0', ',', '0', ',', 'S', ')', '\\n    ', 'return', 'count', '\\n\\n\\n\\n\\n']\n",
      "old : ['\\n', 'def', 'findTargetSumWays(nums', ',', 'S', '):', '\\n    ', 'count', '=', '0', '\\n    ', 'def', 'calculate(nums', ',', 'i', ',', 'sum', ',', 'S', '):', '\\n        ', 'nonlocal', 'count', '\\n        ', 'if', 'i', '=', '=', 'len(nums', '):', '\\n            ', 'if', 'sum', '=', '=', 'S', ':', '\\n                ', 'count', '+', '=', '1', '\\n        ', 'else', ':', '\\n            ', 'calculate(nums', ',', 'i+1', ',', 'sum+', 'nums[i', ']', ',', 'S', ')', '\\n            ', 'calculate(nums', ',', 'i+1', ',', 'sum-', 'nums[i', ']', ',', 'S', ')', '\\n\\n    ', 'calculate(nums', ',', '0', ',', '0', ',', 'S', ')', '\\n    ', 'return', 'count', '\\n\\n\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "print(\"new :\",tokenize_py(val))\n",
    "print(\"old :\",tokenize_en(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
